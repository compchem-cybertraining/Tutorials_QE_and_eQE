{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NACs (HPC version)\n",
    "\n",
    "This file demonstrates how to run the calculations of the NACs in the KS space, using QE.\n",
    "\n",
    "In particular, this example is designed to run calculations on UB HPC cluster, CCR (Center for Computational Research). More specifically, using the nodes of the Akimov group (*valhalla* cluster).\n",
    "\n",
    "Unline for the on-laptop version, we may need to wait until the submitted jobs are done, so we wil not be able to run the plotting and other data analysis calculations right away. So those parts are removed.\n",
    "\n",
    "So, lets start by loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra-plus/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Fisrt, we add the location of the library to test to the PYTHON path\n",
    "if sys.platform==\"cygwin\":\n",
    "    from cyglibra_core import *\n",
    "elif sys.platform==\"linux\" or sys.platform==\"linux2\":\n",
    "    from liblibra_core import *\n",
    "    \n",
    "\n",
    "from libra_py import hpc_utils\n",
    "from libra_py import data_read\n",
    "from libra_py import data_outs\n",
    "from libra_py import units\n",
    "from libra_py import QE_methods\n",
    "from libra_py.workflows.nbra import step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, lets print out the location of the current working directory.\n",
    "\n",
    "This directory should contain a folder called **PP**, in which we should have placed the atomic pseudopotentials suitable for our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/academic/cyberwksp21/Instructors_material/alexeyak/qe/libra_qe/step2a\n"
     ]
    }
   ],
   "source": [
    "print( os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have already produced a QE MD trajectory and it is stored in the file **x0.md.out** (which we copied in the present directory).\n",
    "\n",
    "We need to also create:\n",
    "\n",
    "* a **x0.scf.in** file that contains the parameters for QE calculations (the type of calculation should be *scf*). The file should not contain the atomic coordiantes section, but should contain the cell parameters sections or occupations if they are used. \n",
    "\n",
    "* a **x0.exp.in** file (also to be placed in the present directory). It shall describe the procedured for the wavefunction \"export\" operation - mainly the location and names of atomic pseudopotentials and the correct prefix for the files. \n",
    "\n",
    "In the section below, the user can define (e.g. via copy/paste) the content of the corresponding files and the files will be automatically generated by Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_dir = os.getcwd()+\"/PP/\"\n",
    "\n",
    "scf_in = \"\"\"&CONTROL\n",
    "  calculation = 'scf',\n",
    "  dt = 20.67055,\n",
    "  nstep = 10,\n",
    "  pseudo_dir = '%s',\n",
    "  outdir = './',\n",
    "  prefix = 'x0',\n",
    "  disk_io = 'low',\n",
    "  wf_collect = .true.\n",
    "/\n",
    "\n",
    "&SYSTEM\n",
    "  ibrav = 0,\n",
    "  celldm(1) = 1.89,\n",
    "  nat = 4,\n",
    "  ntyp = 2,\n",
    "  nspin = 2,\n",
    "  starting_magnetization(1) = 0.1,\n",
    "  nbnd = 40,\n",
    "  ecutwfc = 40,\n",
    "  tot_charge = 0.0,\n",
    "  occupations = 'smearing',\n",
    "  smearing = 'gaussian',\n",
    "  degauss = 0.005,\n",
    "  nosym = .true.,\n",
    "/\n",
    "\n",
    "&ELECTRONS\n",
    "  electron_maxstep = 300,\n",
    "  conv_thr = 1.D-5,\n",
    "  mixing_beta = 0.45,\n",
    "/\n",
    "\n",
    "&IONS\n",
    "  ion_dynamics = 'verlet',\n",
    "  ion_temperature = 'andersen',\n",
    "  tempw = 300.00 ,\n",
    "  nraise = 1,\n",
    "/\n",
    "\n",
    "\n",
    "ATOMIC_SPECIES\n",
    " Cd 121.411  Cd.pbe-n-rrkjus_psl.1.0.0.UPF\n",
    " Se 78.96    Se.pbe-dn-rrkjus_psl.1.0.0.UPF\n",
    "\n",
    "\n",
    "K_POINTS automatic\n",
    " 1 1 1 0 0 0\n",
    "\n",
    "CELL_PARAMETERS (alat=  1.89000000)\n",
    "   4.716986504  -0.015512615  -0.002400656\n",
    "  -2.371926710   4.062829845  -0.000273730\n",
    "  -0.002552594  -0.001387965   8.436361230\n",
    "\n",
    "\"\"\" % (PP_dir)\n",
    "\n",
    "exp_in = \"\"\"&inputpp\n",
    "  prefix = 'x0',\n",
    "  outdir = './',\n",
    "  pseudo_dir = '%s',\n",
    "  psfile(1) = 'Cd.pbe-n-rrkjus_psl.1.0.0.UPF',\n",
    "  psfile(2) = 'Se.pbe-dn-rrkjus_psl.1.0.0.UPF',\n",
    "  single_file = .FALSE.,\n",
    "  ascii = .TRUE.,\n",
    "  uspp_spsi = .FALSE.,\n",
    "/\n",
    "\n",
    "\"\"\" % (PP_dir)\n",
    "\n",
    "f = open(\"x0.scf.in\", \"w\")\n",
    "f.write(scf_in)\n",
    "f.close()\n",
    "\n",
    "f = open(\"x0.exp.in\", \"w\")\n",
    "f.write(exp_in)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#print scf_in\n",
    "#print exp_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section will clean up the previous results and temporary directory (BEWARE!!! you may not always want to do this for it will delete expensive results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the previous results and temporary working directory from the previous runs\n",
    "os.system(\"rm -r res\")\n",
    "os.system(\"rm -r wd\")\n",
    "\n",
    "# Create the nelw results directory\n",
    "os.system(\"mkdir res\")\n",
    "rd = os.getcwd()+\"/res\"          # where all the results stuff will go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to setup the submit script template - it will be used to create actual submit scripts (by substituting the param1 and param2 variables) in the working directory. Then those files will be distributed among the job directories and will be used to submit the actual jobs.\n",
    "\n",
    "This section also defines the parameters to be used by the **step2.run()** (and other functions called inside, so look for the description all all suitable parameters). The meaning of the most parameters is quite intuitive. Let me just clarify a couple less-obvious points. \n",
    "\n",
    "That section of the code looks weird because:\n",
    "* it is a Python sctring that defines ...\n",
    "* a SLURM script that uses bash commands and calls ..\n",
    "* the Python script to be executed, which eventually calls ...\n",
    "* the Libra modules to do the step2 calculations\n",
    "\n",
    "In this example, I plan to submit the calculation in the HPC cluster to \"parallelize\" the calculations via the SLURM batch system, so specify \"BATCH_SYSTEM\":\"srun\"\n",
    "\n",
    "The system in this example has 56 electrons, so the HOMO would correspond to number 28 and LUMO to number 29. In this example, we use more orbitals - those below HOMO and above LUMO: minband = 20 and maxband = 39, so we could study the HOMO-LUMO transitions as well as all other types of relaxation. In all, there are 20 orbitals included in our present active space. The resulting files will contain matrixes 40 by 40 - because we plot alpha and beta channels. Just a reminder: the orbital indexing starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_str = \"\"\"#!/bin/sh\n",
    "#SBATCH --partition=valhalla --qos=valhalla\n",
    "#SBATCH --clusters=faculty\n",
    "#SBATCH --time=02:00:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --mem=5000\n",
    "###SBATCH --mail-user=alexeyak@buffalo.edu\n",
    "\n",
    "echo \"SLURM_JOBID=\"$SLURM_JOBID\n",
    "echo \"SLURM_JOB_NODELIST=\"$SLURM_JOB_NODELIST\n",
    "echo \"SLURM_NNODES=\"$SLURM_NNODES\n",
    "echo \"SLURMTMPDIR=\"$SLURMTMPDIR\n",
    "echo \"working directory=\"$SLURM_SUBMIT_DIR\n",
    "\n",
    "NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`\n",
    "echo NPROCS=$NPROCS\n",
    "\n",
    "\n",
    "module load jupyter\n",
    "eval \"$(/projects/academic/cyberwksp21/Software/Conda/Miniconda3/bin/conda shell.bash hook)\"\n",
    "conda activate libra\n",
    "module load espresso/6.2.1\n",
    "export PWSCF=/util/academic/espresso/6.2.1/bin\n",
    "\n",
    "\n",
    "#The PMI library is necessary for srun\n",
    "export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so\n",
    "\n",
    "env\n",
    "which python\n",
    "which pw.x\n",
    "which pw_export.x\n",
    "\n",
    "\n",
    "# These will be assigned automatically, leave them as they are\n",
    "param1=\n",
    "param2=\n",
    "\n",
    "# This is invocation of the scripts which will further handle NA-MD calclculations on the NAC calculation step\n",
    "# NOTE: minband - starting from 1\n",
    "#       maxband - is included\n",
    "\n",
    "python -c \\\"from libra_py.workflows.nbra import step2\n",
    "params = {}\n",
    "params[\\\\\"EXE\\\\\"] = \\\\\"pw.x\\\\\" \n",
    "params[\\\\\"EXE_EXPORT\\\\\"] = \\\\\"pw_export.x\\\\\"\n",
    "params[\\\\\"BATCH_SYSTEM\\\\\"] = \\\\\"srun\\\\\"\n",
    "params[\\\\\"NP\\\\\"] = 4\n",
    "params[\\\\\"start_indx\\\\\"] = $param1\n",
    "params[\\\\\"stop_indx\\\\\"] = $param2\n",
    "params[\\\\\"dt\\\\\"] = %8.5f\n",
    "params[\\\\\"prefix0\\\\\"] = \\\\\"x0.scf\\\\\" \n",
    "params[\\\\\"nac_method\\\\\"] = 1\n",
    "params[\\\\\"minband\\\\\"] = 20\n",
    "params[\\\\\"maxband\\\\\"] = 39\n",
    "params[\\\\\"minband_soc\\\\\"] = 20\n",
    "params[\\\\\"maxband_soc\\\\\"] = 39\n",
    "params[\\\\\"compute_Hprime\\\\\"] = True\n",
    "params[\\\\\"wd\\\\\"] = \\\\\"wd\\\\\"\n",
    "params[\\\\\"rd\\\\\"] = \\\\\"%s\\\\\"\n",
    "params[\\\\\"verbosity\\\\\"] = 0\n",
    "step2.run(params)\n",
    "\\\"\n",
    "\n",
    "\"\"\" % ( 1.0*units.fs2au, os.getcwd()+\"/res\" )\n",
    "\n",
    "\n",
    "f = open(\"submit_templ.slm\", \"w\")\n",
    "f.write(submit_str)\n",
    "f.close()\n",
    "\n",
    "#print submit_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **QE_methods.out2inp()** function to convert the MD trajectory into a bunch of input files for SCF calculation - this is something we'll need for NAC calculations. \n",
    "\n",
    "In this case, you need to setup the *iinit* and *ifinal* variables which determine which steps of the original MD trajectory will be used to produce the input files and subsequenctly used in the NACs calculations\n",
    "\n",
    "All these files will be generated in the temporarily-created **wd** directory. The system then \"cd\" into that directory to start the consecutive operations in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iinit = 0\n",
    "ifinal = 30\n",
    "\n",
    "QE_methods.out2inp(\"x0.md.out\",\"x0.scf.in\",\"wd\",\"x0.scf\", iinit, ifinal,1)\n",
    "\n",
    "os.system(\"cp submit_templ.slm wd\")\n",
    "os.system(\"cp x0.exp.in wd\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets change into our working (temporary) directory, copy all the template files and submit the calculations of multiple jobs. Come back to the original working directory once we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function distribute in module libra_py.hpc_utils:\n",
      "\n",
      "distribute(Nmin, Nmax, max_steps, submit_templ, exp_files, prefixes, do_submit)\n",
      "    This function creates a number of jobs from the pool of input files \n",
      "    which start from ```Nmin``` to ```Nmax``` with the maximal number \n",
      "    of input files in one job given by ```max_steps```.  \n",
      "    It also starts the job in a given directory\n",
      "    \n",
      "    Args: \n",
      "        Nmin ( int ): minimal index of the file (a part of the file name) in the pool\n",
      "        Nmax ( int ): maximal index of the file (a part of the file name) in the pool\n",
      "        max_steps ( int ): how many files to handle in one job\n",
      "        submit_templ ( string ): the filename of the PBS/SLURM submit file that \n",
      "            contains all the information on how to run the jobs involving a subset of\n",
      "            files, but doesn't specify which particular files to handle. These parameters\n",
      "            will be automatically setup by this funciton           \n",
      "        exp_files ( list of strings ): is a list of the input files defining how to do an \n",
      "            export of QE wavefunctions\n",
      "        prefixes ( list of strings ): is a list of prefixes of the files to be distributed\n",
      "        do_submit ( int ): a flag to choose if we actually want to submit the jobs\n",
      "            (do_submit==1 || ==2 || ==3 ) or only distribute the files (otherwise):\n",
      "    \n",
      "            - 0: only discribute files, no actual execution\n",
      "            - 1: distribute and submit using PBS\n",
      "            - 2: distribute and submit using SLURM\n",
      "            - 3: distribute and submit using PYTHON no scheduling systems - good for runs on a local computer\n",
      "    \n",
      "    Returns:\n",
      "        None: just organizes the execution of the calculations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hpc_utils.distribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"wd\")\n",
    "\n",
    "tot_nsteps = 30\n",
    "nsteps_per_job = 10\n",
    "hpc_utils.distribute(0,tot_nsteps,nsteps_per_job,\"submit_templ.slm\",[\"x0.exp.in\"],[\"x0.scf\"],2)\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/academic/cyberwksp21/Instructors_material/alexeyak/qe/libra_qe/step2a\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (libra-latest)",
   "language": "python",
   "name": "libra-plus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
